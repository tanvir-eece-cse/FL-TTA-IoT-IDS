# =============================================================================
# FULL TRAINING CONFIG - L40S GPU (48GB VRAM) MAXIMUM UTILIZATION
# =============================================================================
# Cost: $1.49/hr on Lightning AI
# Expected time: 45-60 minutes for full training
# Expected cost: $1.50-2.00 total
# Expected Results: 94%+ Accuracy, 80%+ Macro F1

# Data - MAXIMUM batch size for L40S 48GB VRAM
batch_size: 32768         # L40S can handle 32K+ for tabular MLP
num_workers: 16           # Max parallel data loading
prefetch_factor: 4        # Prefetch batches for pipeline
persistent_workers: true  # Keep workers alive between epochs
pin_memory: true          # Pin memory for faster GPU transfers
label_column: label_binary
max_flows_per_scenario: 500000  # Memory management for large scenarios

# Model - Deep MLP with Batch Normalization (required for TTA)
model:
  architecture: mlp
  hidden_dims: [1024, 512, 256, 128]  # 4-layer deep network
  dropout: 0.2
  use_batchnorm: true     # REQUIRED for TTA adaptation
  activation: relu
  init: kaiming           # Kaiming initialization

# Training - Optimized for large batch with class weighting
learning_rate: 0.004      # Linear scaling rule: base_lr * batch_size/256
weight_decay: 0.0001
max_epochs: 100
warmup_epochs: 10         # Gradual warmup for large batch stability
patience: 20              # Early stopping patience
gradient_clip: 1.0        # Gradient clipping for stability
class_weighted_loss: true # CRITICAL: Fixes benign class underprediction

# Federated Learning - FedProx with weighted aggregation
federated:
  num_rounds: 80          # Full convergence
  local_epochs: 5         # More local training per round
  proximal_mu: 0.01       # FedProx regularization strength
  fraction_fit: 1.0       # Use all clients each round
  aggregation: fedavg     # Weighted FedAvg

# Test-Time Adaptation - Drift-Aware EATA variant
tta:
  method: drift_aware     # Drift-aware entropy minimization
  lr: 0.00005             # Conservative TTA learning rate
  steps: 1                # Single adaptation step (fast)
  adapt_bn_only: true     # Only adapt BatchNorm layers
  entropy_threshold: 0.28 # Base threshold (adaptive in code)
  fisher_alpha: 2000.0    # Fisher information regularization
  drift_window: 100       # Window for drift detection
  entropy_drift_threshold: 0.3  # Drift detection threshold

# L40S GPU Optimizations - MAXIMUM UTILIZATION
accumulate_grad_batches: 1    # No accumulation needed with 32K batch
precision: bf16-mixed         # BF16 mixed precision (2x throughput)
enable_tf32: true             # TF32 tensor cores (8x faster than FP32)
cudnn_benchmark: true         # Auto-tune convolution algorithms
flash_attention: true         # Memory-efficient attention
compile_model: false          # torch.compile disabled (can be unstable)

# WandB Logging Configuration
wandb:
  project: FL-TTA-IoT-IDS
  entity: null
  tags: ["l40s", "full", "fl", "tta", "class_weighted", "iot23"]
  log_model: true
  log_freq: 5             # Log every 5 rounds
  log_confusion_matrix: true
